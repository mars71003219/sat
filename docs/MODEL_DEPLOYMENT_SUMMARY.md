# ğŸš€ TensorRT ê¸°ë°˜ Triton Inference Server ë°°í¬ ê°€ì´ë“œ

## ğŸ“Œ ì‹œìŠ¤í…œ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **NVIDIA Triton Inference Server**ì™€ **TensorRT**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ GPUì—ì„œ ê³ ì„±ëŠ¥ìœ¼ë¡œ ì„œë¹™í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•

- âœ… **RTX 5060 (sm_120) ì™„ì „ ì§€ì›**: TensorRT ì—”ì§„ ì‚¬ìš©ìœ¼ë¡œ ìµœì‹  GPU í˜¸í™˜ì„± í™•ë³´
- âœ… **ë²”ìš© ëª¨ë¸ ë³€í™˜ ë„êµ¬**: ì–´ë–¤ PyTorch ëª¨ë¸ì´ë“  TensorRTë¡œ ë³€í™˜ ê°€ëŠ¥
- âœ… **ì‚¬ìš©ì ì¹œí™”ì **: ëª¨ë¸ í•™ìŠµ â†’ ë³€í™˜ â†’ ë°°í¬ê¹Œì§€ ìë™í™”
- âœ… **ê³ ì„±ëŠ¥**: FP16 ì •ë°€ë„, Dynamic Batching, CUDA Graph ìµœì í™”

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì‚¬ìš©ì ì›Œí¬í”Œë¡œìš°                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€ 1. PyTorch ëª¨ë¸ í•™ìŠµ
           â”‚    â””â”€ model.pth ìƒì„±
           â”‚
           â”œâ”€ 2. TensorRT ë³€í™˜
           â”‚    â””â”€ python convert_model_to_tensorrt.py
           â”‚         â”œâ”€ ONNX ë³€í™˜
           â”‚         â””â”€ TensorRT ì—”ì§„ ë¹Œë“œ (.plan)
           â”‚
           â”œâ”€ 3. Triton ë°°í¬
           â”‚    â””â”€ model_repository/
           â”‚         â””â”€ my_model/
           â”‚              â”œâ”€ config.pbtxt
           â”‚              â””â”€ 1/
           â”‚                   â””â”€ model.plan
           â”‚
           â””â”€ 4. ì¶”ë¡  ì„œë¹„ìŠ¤
                â””â”€ HTTP/gRPC API

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Triton Inference Server                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ VAE Model    â”‚  â”‚ Transformer  â”‚  â”‚ Custom Model â”‚      â”‚
â”‚  â”‚ (TensorRT)   â”‚  â”‚ (TensorRT)   â”‚  â”‚ (TensorRT)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           Dynamic Batching Engine                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         NVIDIA GPU (RTX 5060, sm_120)              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ êµ¬ì„± ìš”ì†Œ

### 1. ëª¨ë¸ ë³€í™˜ ë„êµ¬

**íŒŒì¼**: `scripts/convert_model_to_tensorrt.py`

**ê¸°ëŠ¥**:
- PyTorch `.pth` íŒŒì¼ ë¡œë“œ
- ONNX ë³€í™˜
- TensorRT ì—”ì§„ ë¹Œë“œ
- Triton ì„¤ì • ìë™ ìƒì„±

**ì‚¬ìš©ë²•**:
```bash
python scripts/convert_model_to_tensorrt.py \
    --model-path YOUR_MODEL.pth \
    --model-class YourModelClass \
    --model-module your_model.py \
    --model-name model_name \
    --input-shape 20,1 \
    --output-shape 10
```

### 2. Triton Server

**ì»¨í…Œì´ë„ˆ**: `nvcr.io/nvidia/tritonserver:24.10-py3`

**ì„¤ì •**:
- TensorRT ë°±ì—”ë“œ
- Dynamic Batching
- GPU ì¸ìŠ¤í„´ìŠ¤
- CUDA Graph ìµœì í™”

### 3. ëª¨ë¸ ì˜ˆì œ

#### VAE (Variational Autoencoder)

```python
# model_repository/vae_timeseries/1/train_and_export.py
class TimeSeriesVAE(nn.Module):
    - ì…ë ¥: [batch, 1, 20] (1ì±„ë„, 20 timesteps)
    - ì¶œë ¥: [batch, 10] (10 forecast steps)
    - Latent Dim: 32
    - Architecture: Encoder-Decoder with reparameterization
```

#### Transformer

```python
# model_repository/transformer_timeseries/1/train_and_export.py
class TimeSeriesTransformer(nn.Module):
    - ì…ë ¥: [batch, 20, 1] (20 timesteps, 1 feature)
    - ì¶œë ¥: [batch, 10] (10 forecast steps)
    - d_model: 64
    - nhead: 4
    - num_layers: 3
    - Architecture: Multi-head self-attention
```

---

## ğŸ“‹ ë¹ ë¥¸ ì‹œì‘

### Step 1: ëª¨ë¸ í•™ìŠµ

```python
import torch
import torch.nn as nn

# ëª¨ë¸ ì •ì˜
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        # ... ëª¨ë¸ ì •ì˜ ...

    def forward(self, x):
        # ... forward ë¡œì§ ...
        return output

# í•™ìŠµ
model = MyModel()
# ... í•™ìŠµ ì½”ë“œ ...

# ì €ì¥
torch.save({
    'model_state_dict': model.state_dict(),
    'model_config': {...}  # ì˜µì…˜
}, 'my_model.pth')
```

### Step 2: TensorRT ë³€í™˜

```bash
python scripts/convert_model_to_tensorrt.py \
    --model-path my_model.pth \
    --model-class MyModel \
    --model-module my_model.py \
    --model-name my_timeseries \
    --input-shape 20,1 \
    --output-shape 10 \
    --output-dir ./model_repository
```

**ì¶œë ¥**:
```
model_repository/
â””â”€â”€ my_timeseries/
    â”œâ”€â”€ config.pbtxt
    â””â”€â”€ 1/
        â””â”€â”€ model.plan
```

### Step 3: Triton ë°°í¬

```bash
# 1. ëª¨ë¸ ë³µì‚¬
cp -r ./model_repository/my_timeseries /path/to/satellite/model_repository/

# 2. Triton Server ì‹œì‘
cd /path/to/satellite
docker compose up -d triton-server

# 3. ëª¨ë¸ í™•ì¸
curl http://localhost:8500/v2/models/my_timeseries
```

### Step 4: ì¶”ë¡  í…ŒìŠ¤íŠ¸

```bash
# Python í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
python scripts/test_simulator.py
```

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
satellite/
â”œâ”€â”€ Dockerfile.triton                      # Triton Server Dockerfile (ê°„ì†Œí™”)
â”œâ”€â”€ docker-compose.yml                     # Docker Compose ì„¤ì •
â”‚
â”œâ”€â”€ model_repository/                      # Triton ëª¨ë¸ ë¦¬í¬ì§€í† ë¦¬
â”‚   â”œâ”€â”€ vae_timeseries/
â”‚   â”‚   â”œâ”€â”€ config.pbtxt
â”‚   â”‚   â””â”€â”€ 1/
â”‚   â”‚       â”œâ”€â”€ model.plan                 # TensorRT ì—”ì§„
â”‚   â”‚       â”œâ”€â”€ model.onnx                 # ONNX (ì¤‘ê°„ íŒŒì¼, ì˜µì…˜)
â”‚   â”‚       â””â”€â”€ train_and_export.py        # í•™ìŠµ/ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”‚
â”‚   â””â”€â”€ transformer_timeseries/
â”‚       â”œâ”€â”€ config.pbtxt
â”‚       â””â”€â”€ 1/
â”‚           â”œâ”€â”€ model.plan
â”‚           â””â”€â”€ train_and_export.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ convert_model_to_tensorrt.py       # â­ í•µì‹¬ ë³€í™˜ ë„êµ¬
â”‚   â”œâ”€â”€ demo_create_simple_model.py        # í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ì˜ˆì œ
â”‚   â””â”€â”€ README_MODEL_CONVERSION.md         # â­ ìƒì„¸ ê°€ì´ë“œ
â”‚
â”œâ”€â”€ shared/
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ inference_response.py          # API ìŠ¤í‚¤ë§ˆ
â”‚
â””â”€â”€ MODEL_DEPLOYMENT_SUMMARY.md            # ì´ íŒŒì¼
```

---

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### 1. ë°°ì¹˜ í¬ê¸° ìµœì í™”

```bash
python scripts/convert_model_to_tensorrt.py \
    --min-batch 1 \
    --opt-batch 16 \  # ê°€ì¥ ë§ì´ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°
    --max-batch 64
```

### 2. ì •ë°€ë„ ì„ íƒ

```bash
# FP16 (ê¸°ë³¸, ê¶Œì¥)
--fp16

# FP32 (ì •í™•ë„ ìš°ì„ )
--no-fp16

# INT8 (ìµœê³  ì„±ëŠ¥, ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš”)
--int8
```

### 3. GPU ì¸ìŠ¤í„´ìŠ¤ ì¡°ì •

```bash
# ê²½ëŸ‰ ëª¨ë¸: ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
--instance-count 4

# ë¬´ê±°ìš´ ëª¨ë¸ (Transformer): 1ê°œ
--instance-count 1
```

### 4. config.pbtxt ìˆ˜ë™ ìˆ˜ì •

```protobuf
# Dynamic Batching ì¡°ì •
dynamic_batching {
  preferred_batch_size: [ 4, 8, 16, 32 ]
  max_queue_delay_microseconds: 100000  # 100ms
}

# CUDA Graph ìµœì í™”
optimization {
  cuda {
    graphs: true
    graph_spec {
      batch_size: 8
      input { ... }
    }
  }
}
```

---

## ğŸš¨ ë¬¸ì œ í•´ê²°

### 1. RTX 5060 í˜¸í™˜ì„±

**ë¬¸ì œ**: PyTorchê°€ sm_120ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ

**í•´ê²°**: âœ… TensorRT ì‚¬ìš©ìœ¼ë¡œ í•´ê²°ë¨
- TensorRTëŠ” ë¹Œë“œ ì‹œ í˜„ì¬ GPUì— ìµœì í™”ëœ ì—”ì§„ ìƒì„±
- RTX 5060ì—ì„œ ì™„ë²½ ì‘ë™

### 2. trtexec ëª…ë ¹ ì—†ìŒ

**í•´ê²°**:
```bash
# Docker ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ ì‹¤í–‰
docker run --gpus all -v $(pwd):/workspace \
    nvcr.io/nvidia/tritonserver:24.10-py3 \
    bash -c "cd /workspace && python scripts/convert_model_to_tensorrt.py ..."
```

### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±

**í•´ê²°**:
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
--max-batch 16

# ì¸ìŠ¤í„´ìŠ¤ ê°œìˆ˜ ì¤„ì´ê¸°
--instance-count 1

# FP16 ì‚¬ìš©
--fp16
```

### 4. ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨

**í™•ì¸ì‚¬í•­**:
```python
# ëª¨ë¸ í´ë˜ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
# state_dict í‚¤ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
checkpoint = torch.load('model.pth')
print(checkpoint.keys())
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì˜ˆìƒ ì„±ëŠ¥ (RTX 5060)

| ëª¨ë¸ | Batch Size | Latency (ms) | Throughput (RPS) |
|------|-----------|--------------|------------------|
| VAE (FP16) | 16 | ~5-10 | ~1000-2000 |
| Transformer (FP16) | 8 | ~10-20 | ~400-800 |
| Simple LSTM (FP16) | 32 | ~3-8 | ~2000-4000 |

*ì‹¤ì œ ì„±ëŠ¥ì€ ëª¨ë¸ í¬ê¸°ì™€ ë³µì¡ë„ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤*

### ìµœì í™” íŒ

1. **Dynamic Batching í™œìš©**: ì—¬ëŸ¬ ìš”ì²­ì„ ìë™ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
2. **FP16 ì •ë°€ë„**: ì•½ 2ë°° ì„±ëŠ¥ í–¥ìƒ, ì •í™•ë„ ê±°ì˜ ë™ì¼
3. **CUDA Graph**: ì»¤ë„ ì‹¤í–‰ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
4. **Optimal Batch Size**: ì‹œìŠ¤í…œ ë¶€í•˜ì— ë§ê²Œ ì¡°ì •

---

## ğŸ”„ ì›Œí¬í”Œë¡œìš° ë¹„êµ

### âŒ ì´ì „ ë°©ì‹ (PyTorch ë°±ì—”ë“œ)

```
PyTorch ëª¨ë¸ (.pth)
    â†“
Triton Python Backend
    â†“
GPU í˜¸í™˜ì„± ë¬¸ì œ (sm_120 ë¯¸ì§€ì›)
    â†“
âŒ ì‹¤íŒ¨
```

### âœ… í˜„ì¬ ë°©ì‹ (TensorRT ë°±ì—”ë“œ)

```
PyTorch ëª¨ë¸ (.pth)
    â†“
ONNX ë³€í™˜
    â†“
TensorRT ì—”ì§„ ë¹Œë“œ (.plan)
    â†“
Triton TensorRT Backend
    â†“
âœ… RTX 5060ì—ì„œ ì™„ë²½ ì‘ë™
```

---

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ê³µì‹ ë¬¸ì„œ

- [TensorRT ë¬¸ì„œ](https://docs.nvidia.com/deeplearning/tensorrt/)
- [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/)
- [ONNX](https://onnx.ai/)

### ë‚´ë¶€ ê°€ì´ë“œ

- `scripts/README_MODEL_CONVERSION.md` - ìƒì„¸í•œ ë³€í™˜ ê°€ì´ë“œ
- `model_repository/vae_timeseries/1/train_and_export.py` - VAE ì˜ˆì œ
- `model_repository/transformer_timeseries/1/train_and_export.py` - Transformer ì˜ˆì œ

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ìƒˆ ëª¨ë¸ ë°°í¬ ì‹œ:

- [ ] PyTorch ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
- [ ] `.pth` íŒŒì¼ ì €ì¥ (state_dict + config ê¶Œì¥)
- [ ] `convert_model_to_tensorrt.py` ì‹¤í–‰
- [ ] ìƒì„±ëœ `model.plan` í™•ì¸
- [ ] `config.pbtxt` ê²€í†  (í•„ìš”ì‹œ ìˆ˜ì •)
- [ ] Triton ëª¨ë¸ ë¦¬í¬ì§€í† ë¦¬ë¡œ ë³µì‚¬
- [ ] Triton Server ì¬ì‹œì‘
- [ ] ì¶”ë¡  í…ŒìŠ¤íŠ¸ (`curl` ë˜ëŠ” í´ë¼ì´ì–¸íŠ¸ ìŠ¤í¬ë¦½íŠ¸)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

---

## ğŸ¯ ê²°ë¡ 

ì´ ì‹œìŠ¤í…œì€ **ìœ ì—°ì„±**ê³¼ **ì„±ëŠ¥**ì„ ëª¨ë‘ ê°–ì¶˜ í”„ë¡œë•ì…˜ ë ˆë²¨ ì¶”ë¡  ì„œë²„ì…ë‹ˆë‹¤:

1. **ë²”ìš©ì„±**: ì–´ë–¤ PyTorch ëª¨ë¸ì´ë“  TensorRTë¡œ ë³€í™˜ ê°€ëŠ¥
2. **ìµœì‹  GPU ì§€ì›**: RTX 5060 (sm_120) ì™„ë²½ ì§€ì›
3. **ì‚¬ìš©ì ì¹œí™”ì„±**: í•œ ì¤„ ëª…ë ¹ìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ
4. **ê³ ì„±ëŠ¥**: FP16, Dynamic Batching, CUDA Graph ìµœì í™”
5. **í™•ì¥ì„±**: ìƒˆ ëª¨ë¸ ì¶”ê°€ê°€ ë§¤ìš° ê°„ë‹¨

**í•µì‹¬ ëª…ë ¹ í•œ ì¤„**:

```bash
python scripts/convert_model_to_tensorrt.py \
    --model-path YOUR_MODEL.pth \
    --model-class YOUR_CLASS \
    --model-module YOUR_MODULE.py \
    --model-name YOUR_NAME \
    --input-shape X,Y \
    --output-shape Z
```

ì´ì œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì´ë‚˜ ëª¨ë¸ì´ ìƒê¸°ë©´, ìœ„ ëª…ë ¹ í•˜ë‚˜ë¡œ ì¦‰ì‹œ Tritonì— ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
