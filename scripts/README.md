# Triton μ¶”λ΅  ν”λ«νΌ - λ¨λΈ λ³€ν™ κ°€μ΄λ“

μ΄ λ””λ ‰ν† λ¦¬λ” **μ™Έλ¶€ ν•™μµ μ„λ²„μ—μ„ λ°›μ€ λ¨λΈ**μ„ Triton Inference Serverμ©μΌλ΅ λ³€ν™ν•λ” λ„κµ¬λ¥Ό ν¬ν•¨ν•©λ‹λ‹¤.

## π“ νμΌ κµ¬μ΅°

```
scripts/
β”β”€β”€ convert_model.py          # λ²”μ© λ¨λΈ λ³€ν™ λ„κµ¬ (ONNX/TensorRT)
β”β”€β”€ convert_example.sh         # λ³€ν™ μμ‹ μ¤ν¬λ¦½νΈ
β”β”€β”€ test_simulator.py          # μ „μ²΄ ν…μ¤νΈ μ¤μ„νΈ
β”β”€β”€ test_single_model.py       # λ‹¨μΌ λ¨λΈ ν…μ¤νΈ
β””β”€β”€ README.md                  # μ΄ λ¬Έμ„
```

## π”„ μ›ν¬ν”λ΅μ°

### 1. μ™Έλ¶€ ν•™μµ μ„λ²„μ—μ„ λ¨λΈ λ°›κΈ°

μ™Έλ¶€ ν•™μµ ν€μΌλ΅λ¶€ν„° λ‹¤μμ„ λ°›μµλ‹λ‹¤:
- `.pth` νμΌ (PyTorch κ°€μ¤‘μΉ)
- λ¨λΈ ν΄λμ¤ μ •μ (`.py` νμΌ)
- μ…λ ¥/μ¶λ ¥ shape μ •λ³΄

### 2. λ¨λΈ λ³€ν™

#### Option A: ONNX Runtime λ°±μ—”λ“ (κ¶μ¥ - μµκ³  GPU νΈν™μ„±)

```bash
python3 convert_model.py \
    --model-path /path/to/model.pth \
    --model-class YourModelClass \
    --model-module your_model_definition.py \
    --model-name your_model_name \
    --input-shape <batch,channels,sequence> \
    --output-shape <forecast_steps> \
    --backend onnx \
    --output-dir /workspace/model_repository
```

#### Option B: TensorRT λ°±μ—”λ“ (μµκ³  μ„±λ¥, GPU νΈν™μ„± μ£Όμ)

```bash
python3 convert_model.py \
    --model-path /path/to/model.pth \
    --model-class YourModelClass \
    --model-module your_model_definition.py \
    --model-name your_model_name \
    --input-shape <batch,channels,sequence> \
    --output-shape <forecast_steps> \
    --backend tensorrt \
    --output-dir /workspace/model_repository
```

### 3. Triton Server μ¬μ‹μ‘

```bash
docker compose restart triton-server
```

### 4. ν…μ¤νΈ

```bash
# λ‹¨μΌ λ¨λΈ λΉ λ¥Έ ν…μ¤νΈ
python3 test_single_model.py

# μ „μ²΄ ν…μ¤νΈ μ¤μ„νΈ
python3 test_simulator.py
```

## π“ convert_model.py νλΌλ―Έν„° μ„¤λ…

| νλΌλ―Έν„° | μ„¤λ… | μμ‹ |
|---------|------|------|
| `--model-path` | PyTorch λ¨λΈ νμΌ κ²½λ΅ | `vae_model.pth` |
| `--model-class` | λ¨λΈ ν΄λμ¤ μ΄λ¦„ | `TimeSeriesVAE` |
| `--model-module` | λ¨λΈ μ •μ νμ΄μ¬ νμΌ | `vae_model.py` |
| `--model-name` | Tritonμ—μ„ μ‚¬μ©ν•  λ¨λΈ μ΄λ¦„ | `vae_timeseries` |
| `--input-shape` | λ¨λΈ μ…λ ¥ shape (λ°°μΉ μ μ™Έ) | `1,20` |
| `--output-shape` | λ¨λΈ μ¶λ ¥ shape (λ°°μΉ μ μ™Έ) | `10` |
| `--backend` | λ°±μ—”λ“ μ„ νƒ (`onnx` λλ” `tensorrt`) | `onnx` |
| `--output-dir` | μ¶λ ¥ λ””λ ‰ν† λ¦¬ | `../model_repository` |

## π― μ‹¤μ  μ‚¬μ© μμ‹

### VAE λ¨λΈ λ³€ν™

```bash
python3 convert_model.py \
    --model-path /external/vae_timeseries.pth \
    --model-class TimeSeriesVAE \
    --model-module vae_definition.py \
    --model-name vae_timeseries \
    --input-shape 1,20 \
    --output-shape 10 \
    --backend onnx \
    --output-dir ../model_repository
```

### Transformer λ¨λΈ λ³€ν™

```bash
python3 convert_model.py \
    --model-path /external/transformer_timeseries.pth \
    --model-class TimeSeriesTransformer \
    --model-module transformer_definition.py \
    --model-name transformer_timeseries \
    --input-shape 20,1 \
    --output-shape 10 \
    --backend onnx \
    --output-dir ../model_repository
```

## π”§ GPU νΈν™μ„±

### ONNX Runtime (κ¶μ¥)
- β… RTX 5060 (sm_100) νΈν™
- β… λ€λ¶€λ¶„μ NVIDIA GPU μ§€μ›
- β… CPU fallback μ§€μ›

### TensorRT
- β οΈ GPUλ³„ μ—”μ§„ λΉλ“ ν•„μ”
- β οΈ RTX 5060 (sm_100)μ€ μµμ‹  TensorRT ν•„μ”
- β… μµκ³  μ„±λ¥

## π“ μ°Έκ³  λ¬Έμ„

- [Triton Inference Server κ³µμ‹ λ¬Έμ„](https://docs.nvidia.com/deeplearning/triton-inference-server/)
- [ONNX Runtime λ¬Έμ„](https://onnxruntime.ai/docs/)
- [TensorRT λ¬Έμ„](https://docs.nvidia.com/deeplearning/tensorrt/)

## β οΈ μ¤‘μ” μ‚¬ν•­

1. **μ΄ ν”λ«νΌμ€ μ¶”λ΅  μ „μ©μ…λ‹λ‹¤** - λ¨λΈ ν•™μµμ€ μ™Έλ¶€ μ„λ²„μ—μ„ μν–‰
2. **λ¨λΈ μ •μ νμΌ ν•„μ”** - `.pth` νμΌλ§μΌλ΅λ” λ³€ν™ λ¶κ°€λ¥
3. **GPU νΈν™μ„± ν™•μΈ** - TensorRT μ‚¬μ© μ‹ GPU compute capability ν™•μΈ
4. **μ…λ ¥/μ¶λ ¥ shape μ •ν™•μ„±** - λ³€ν™ μ‹ μ •ν™•ν• shape μ •λ³΄ ν•„μ
