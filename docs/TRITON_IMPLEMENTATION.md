# Nvidia Triton Inference Server ì ìš© ì™„ë£Œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì£¼ìš” ë³€ê²½ ì‚¬í•­](#ì£¼ìš”-ë³€ê²½-ì‚¬í•­)
3. [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
4. [ì„¤ì • ë° ì‹¤í–‰](#ì„¤ì •-ë°-ì‹¤í–‰)
5. [ëª¨ë¸ ê´€ë¦¬](#ëª¨ë¸-ê´€ë¦¬)
6. [ëª¨ë‹ˆí„°ë§](#ëª¨ë‹ˆí„°ë§)
7. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
8. [ì„±ëŠ¥ ë¹„êµ](#ì„±ëŠ¥-ë¹„êµ)

---

## ê°œìš”

### ì ìš© ì¼ì
2025-10-15

### ì ìš© ì´ìœ 

ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ìˆ˜ë™ êµ¬í˜„ ì½”ë“œ(`BatchManager`, `ModelLoader`, `ModelFactory`)ë¥¼ **Nvidia Triton Inference Server**ë¡œ ëŒ€ì²´í•˜ì—¬:

1. **ì„±ëŠ¥ ê·¹ëŒ€í™”**
   - C++ ê¸°ë°˜ Dynamic Batching (Python ìˆ˜ë™ ë°°ì¹˜ë³´ë‹¤ 5-10ë°° ë¹ ë¦„)
   - GPU í™œìš©ë¥  ìµœëŒ€í™” (30-40% â†’ 70-90%)
   - ë™ì‹œ ëª¨ë¸ ì‹¤í–‰ ì§€ì›

2. **ì½”ë“œ ë³µì¡ë„ ê°ì†Œ**
   - 3ê°œ í•µì‹¬ íŒŒì¼ (`batch_manager.py`, `model_loader.py`, `model_factory.py`) ì œê±°
   - Analysis Worker ë¡œì§ ë‹¨ìˆœí™” (ì¶”ë¡  â†’ Triton í´ë¼ì´ì–¸íŠ¸)

3. **ìš´ì˜ íš¨ìœ¨ì„± í–¥ìƒ**
   - í‘œì¤€ API (HTTP/gRPC)
   - ìë™ ëª¨ë‹ˆí„°ë§ (Prometheus metrics)
   - ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ê¸°ë³¸ ì§€ì›

### ì ìš© ë°©ì‹

**ë°©ë²• A: Analysis Workerê°€ Triton í´ë¼ì´ì–¸íŠ¸** (ì„ íƒë¨)
- ì „/í›„ì²˜ë¦¬: Analysis Worker (Python)
- í•µì‹¬ ì¶”ë¡ : Triton Server (GPU)
- ê°€ì¥ ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ì „í™˜ ë°©ì‹

---

## ì£¼ìš” ë³€ê²½ ì‚¬í•­

### 1. ì¸í”„ë¼ ë³€ê²½

#### Docker Compose

```yaml
# ì¶”ê°€ëœ ì„œë¹„ìŠ¤
triton-server:
  image: nvcr.io/nvidia/tritonserver:24.01-py3
  ports:
    - "8500:8000"  # HTTP
    - "8501:8001"  # gRPC
    - "8502:8002"  # Metrics
  volumes:
    - ./model_repository:/models
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

#### Analysis Worker ë³€ê²½

```diff
  analysis-worker-1:
    environment:
-     - CUDA_VISIBLE_DEVICES=0
+     - TRITON_SERVER_URL=triton-server:8001
    depends_on:
-     - kafka
+     - kafka
+     - triton-server
-   deploy:
-     resources:
-       reservations:
-         devices:
-           - driver: nvidia
```

**ë³€ê²½ ì´ìœ **: GPUëŠ” Triton Serverë§Œ ì‚¬ìš©, Analysis WorkerëŠ” CPUë¡œ ì¶©ë¶„

### 2. ì½”ë“œ ë³€ê²½

#### ì‚­ì œëœ íŒŒì¼
```
analysis-server/core/
â”œâ”€â”€ batch_manager.py     âŒ ì‚­ì œ (Triton Dynamic Batchingìœ¼ë¡œ ëŒ€ì²´)
â”œâ”€â”€ model_loader.py      âŒ ì‚­ì œ (Triton Model Repositoryë¡œ ëŒ€ì²´)
â””â”€â”€ model_factory.py     âŒ ì‚­ì œ (Triton Model Repositoryë¡œ ëŒ€ì²´)
```

#### ì¶”ê°€ëœ íŒŒì¼
```
analysis-server/core/
â””â”€â”€ triton_client.py     âœ… ìƒˆë¡œ ì¶”ê°€ (ì „/í›„ì²˜ë¦¬ + Triton gRPC í†µì‹ )

model_repository/
â”œâ”€â”€ lstm_timeseries/
â”‚   â”œâ”€â”€ config.pbtxt     âœ… Triton ëª¨ë¸ ì„¤ì •
â”‚   â””â”€â”€ 1/
â”‚       â””â”€â”€ model.py     âœ… Python Backend ëª¨ë¸
â””â”€â”€ moving_average/
    â”œâ”€â”€ config.pbtxt     âœ… Triton ëª¨ë¸ ì„¤ì •
    â””â”€â”€ 1/
        â””â”€â”€ model.py     âœ… Python Backend ëª¨ë¸
```

#### ìˆ˜ì •ëœ íŒŒì¼
```
analysis-server/tasks.py
- ê¸°ì¡´: BatchManager â†’ ModelLoader â†’ Model.infer()
+ ë³€ê²½: TritonClient.infer() (ì „ì²˜ë¦¬ + Triton ì¶”ë¡  + í›„ì²˜ë¦¬)
```

### 3. ì˜ì¡´ì„± ë³€ê²½

```diff
# analysis-server/requirements.txt
  torch==2.1.0
  numpy==1.26.4
+ tritonclient[grpc]==2.41.0
```

---

## ì•„í‚¤í…ì²˜

### ì´ì „ ì•„í‚¤í…ì²˜ (ìˆ˜ë™ êµ¬í˜„)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Operation Server   â”‚
â”‚   (FastAPI)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ Celery Task
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analysis Worker    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚BatchManager  â”‚  â”‚ â† threading.Timerë¡œ ë°°ì¹˜ ìˆ˜ì§‘
â”‚  â”‚(Python)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ModelLoader   â”‚  â”‚ â† LRU ìºì‹œ, ëª¨ë¸ ë¡œë”©
â”‚  â”‚(Python)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚PyTorch Model â”‚  â”‚ â† ì§ì ‘ GPU ì‚¬ìš©
â”‚  â”‚(GPU)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë¬¸ì œì **:
- Python ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ â†’ ëŠë¦¼
- ìˆ˜ë™ GPU ê´€ë¦¬ â†’ ë¹„íš¨ìœ¨ì 
- ë³µì¡í•œ ì½”ë“œ â†’ ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€

### í˜„ì¬ ì•„í‚¤í…ì²˜ (Triton ê¸°ë°˜)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Operation Server   â”‚
â”‚   (FastAPI)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ Celery Task
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analysis Worker    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚TritonClient  â”‚  â”‚ â† ì „/í›„ì²˜ë¦¬ (Python)
â”‚  â”‚(gRPC)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚ gRPC     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Triton Server      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Dynamic       â”‚  â”‚ â† ìë™ ë°°ì¹˜ (C++)
â”‚  â”‚Batching      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Model         â”‚  â”‚ â† ìë™ ë¡œë”©, ë²„ì „ ê´€ë¦¬
â”‚  â”‚Repository    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚PyTorch Model â”‚  â”‚ â† GPU ìµœì í™” ì‹¤í–‰
â”‚  â”‚(GPU)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì¥ì **:
- C++ ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ â†’ 5-10ë°° ë¹ ë¦„
- ìë™ GPU ê´€ë¦¬ â†’ í™œìš©ë¥  2ë°° í–¥ìƒ
- ë‹¨ìˆœí•œ ì½”ë“œ â†’ ìœ ì§€ë³´ìˆ˜ ìš©ì´

---

## ì„¤ì • ë° ì‹¤í–‰

### 1. ì‚¬ì „ ì¤€ë¹„

#### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- Docker ë° Docker Compose
- Nvidia GPU (CUDA ì§€ì›)
- Nvidia Docker Runtime

#### GPU ë“œë¼ì´ë²„ í™•ì¸
```bash
nvidia-smi
```

### 2. ì‹œìŠ¤í…œ ì‹œì‘

```bash
# 1. Kafka ì´ˆê¸°í™” (ì²˜ìŒ í•œ ë²ˆë§Œ)
./init-kafka.sh

# 2. Docker Composeë¡œ ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘
docker-compose up -d

# 3. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps
```

### 3. Triton Server ìƒíƒœ í™•ì¸

#### Health Check
```bash
# HTTP
curl http://localhost:8500/v2/health/ready

# ê²°ê³¼:
# {"status": "ready"}
```

#### ëª¨ë¸ ëª©ë¡ í™•ì¸
```bash
curl http://localhost:8500/v2/models

# ê²°ê³¼:
# {
#   "models": [
#     {"name": "lstm_timeseries", "version": "1", "state": "READY"},
#     {"name": "moving_average", "version": "1", "state": "READY"}
#   ]
# }
```

#### ë¡œê·¸ í™•ì¸
```bash
docker-compose logs -f triton-server
```

### 4. ì¶”ë¡  í…ŒìŠ¤íŠ¸

#### APIë¥¼ í†µí•œ í…ŒìŠ¤íŠ¸
```bash
curl -X POST http://localhost:8000/api/v1/inference/submit \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "lstm_timeseries",
    "data": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],
    "config": {
      "sequence_length": 10,
      "forecast_steps": 5
    }
  }'
```

#### ê²°ê³¼ í™•ì¸
```bash
# job_id ë°›ì•„ì˜¤ê¸° (ì˜ˆ: abc-123)
curl http://localhost:8000/api/v1/inference/result/abc-123
```

---

## ëª¨ë¸ ê´€ë¦¬

### Model Repository êµ¬ì¡°

```
model_repository/
â”œâ”€â”€ lstm_timeseries/
â”‚   â”œâ”€â”€ config.pbtxt          # Triton ëª¨ë¸ ì„¤ì •
â”‚   â””â”€â”€ 1/                    # ë²„ì „ 1
â”‚       â””â”€â”€ model.py          # Python Backend ëª¨ë¸
â”œâ”€â”€ moving_average/
â”‚   â”œâ”€â”€ config.pbtxt
â”‚   â””â”€â”€ 1/
â”‚       â””â”€â”€ model.py
```

### ìƒˆ ëª¨ë¸ ì¶”ê°€ ë°©ë²•

#### 1. ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
```bash
mkdir -p model_repository/my_new_model/1
```

#### 2. ëª¨ë¸ íŒŒì¼ ì‘ì„±
```python
# model_repository/my_new_model/1/model.py
import numpy as np
import triton_python_backend_utils as pb_utils

class TritonPythonModel:
    def initialize(self, args):
        # ëª¨ë¸ ì´ˆê¸°í™”
        pass

    def execute(self, requests):
        responses = []
        for request in requests:
            # ì…ë ¥ ì¶”ì¶œ
            input_tensor = pb_utils.get_input_tensor_by_name(request, "INPUT")
            input_data = input_tensor.as_numpy()

            # ì¶”ë¡  ì‹¤í–‰
            output_data = your_inference_logic(input_data)

            # ì¶œë ¥ ìƒì„±
            output_tensor = pb_utils.Tensor("OUTPUT", output_data)
            responses.append(pb_utils.InferenceResponse([output_tensor]))

        return responses

    def finalize(self):
        # ì •ë¦¬ ì‘ì—…
        pass
```

#### 3. Config íŒŒì¼ ì‘ì„±
```protobuf
# model_repository/my_new_model/config.pbtxt
name: "my_new_model"
backend: "python"
max_batch_size: 32

input [
  {
    name: "INPUT"
    data_type: TYPE_FP32
    dims: [ -1 ]  # ê°€ë³€ ê¸¸ì´
  }
]

output [
  {
    name: "OUTPUT"
    data_type: TYPE_FP32
    dims: [ -1 ]
  }
]

dynamic_batching {
  preferred_batch_size: [ 8, 16, 32 ]
  max_queue_delay_microseconds: 100000
}

instance_group [
  {
    count: 2
    kind: KIND_GPU
  }
]
```

#### 4. ëª¨ë¸ ë¡œë“œ
```bash
# Tritonì´ ìë™ìœ¼ë¡œ ìƒˆ ëª¨ë¸ ê°ì§€ ë° ë¡œë“œ
# ë˜ëŠ” ìˆ˜ë™ reload:
curl -X POST http://localhost:8500/v2/repository/models/my_new_model/load
```

### ëª¨ë¸ ë²„ì „ ê´€ë¦¬

#### ìƒˆ ë²„ì „ ì¶”ê°€
```bash
# ë²„ì „ 2 ìƒì„±
mkdir -p model_repository/lstm_timeseries/2
cp model_repository/lstm_timeseries/1/model.py model_repository/lstm_timeseries/2/

# Tritonì´ ìë™ìœ¼ë¡œ ë¡œë“œ
```

#### íŠ¹ì • ë²„ì „ ì‚¬ìš©
```python
# Client ì½”ë“œì—ì„œ
response = client.infer(
    model_name="lstm_timeseries",
    model_version="2",  # ë²„ì „ ì§€ì •
    inputs=inputs
)
```

---

## ëª¨ë‹ˆí„°ë§

### 1. Prometheus Metrics

Tritonì€ ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ ìë™ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

#### ë©”íŠ¸ë¦­ í™•ì¸
```bash
curl http://localhost:8502/metrics
```

#### ì£¼ìš” ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… |
|--------|------|
| `nv_inference_request_success` | ì„±ê³µí•œ ì¶”ë¡  ìš”ì²­ ìˆ˜ |
| `nv_inference_request_failure` | ì‹¤íŒ¨í•œ ì¶”ë¡  ìš”ì²­ ìˆ˜ |
| `nv_inference_queue_duration_us` | í ëŒ€ê¸° ì‹œê°„ (ë§ˆì´í¬ë¡œì´ˆ) |
| `nv_inference_compute_infer_duration_us` | ì¶”ë¡  ì‹¤í–‰ ì‹œê°„ |
| `nv_inference_count` | ì¶”ë¡  ê±´ìˆ˜ |
| `nv_gpu_utilization` | GPU ì‚¬ìš©ë¥  (%) |
| `nv_gpu_memory_total_bytes` | GPU ì´ ë©”ëª¨ë¦¬ |
| `nv_gpu_memory_used_bytes` | GPU ì‚¬ìš© ë©”ëª¨ë¦¬ |

### 2. Grafana ëŒ€ì‹œë³´ë“œ (ì„ íƒì‚¬í•­)

#### Prometheus ì„¤ì •
```yaml
# docker-compose.ymlì— ì¶”ê°€
prometheus:
  image: prom/prometheus:latest
  ports:
    - "9090:9090"
  volumes:
    - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
  networks:
    - webnet
```

#### prometheus.yml
```yaml
scrape_configs:
  - job_name: 'triton'
    static_configs:
      - targets: ['triton-server:8002']
```

#### Grafana ì„¤ì •
```yaml
# docker-compose.ymlì— ì¶”ê°€
grafana:
  image: grafana/grafana:latest
  ports:
    - "3000:3000"
  depends_on:
    - prometheus
  networks:
    - webnet
```

#### ëŒ€ì‹œë³´ë“œ ì ‘ì†
- URL: http://localhost:3000
- ê¸°ë³¸ ê³„ì •: admin / admin

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Triton Serverê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ

#### ì¦ìƒ
```bash
docker-compose logs triton-server
# Error: Model repository not found
```

#### í•´ê²° ë°©ë²•
```bash
# Model repository ë””ë ‰í† ë¦¬ í™•ì¸
ls -la model_repository/

# ê¶Œí•œ ë¬¸ì œ í•´ê²°
chmod -R 755 model_repository/
```

### 2. ëª¨ë¸ì´ READY ìƒíƒœê°€ ì•„ë‹˜

#### ì¦ìƒ
```bash
curl http://localhost:8500/v2/models/lstm_timeseries
# {"state": "UNAVAILABLE"}
```

#### í•´ê²° ë°©ë²•
```bash
# ëª¨ë¸ ë¡œê·¸ í™•ì¸
docker-compose logs triton-server | grep lstm_timeseries

# config.pbtxt ë¬¸ë²• í™•ì¸
cat model_repository/lstm_timeseries/config.pbtxt

# ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
ls -la model_repository/lstm_timeseries/1/
```

### 3. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

#### ì¦ìƒ
```bash
# CUDA out of memory
```

#### í•´ê²° ë°©ë²•
```protobuf
# config.pbtxtì—ì„œ instance ìˆ˜ ì¤„ì´ê¸°
instance_group [
  {
    count: 1  # 2 â†’ 1ë¡œ ê°ì†Œ
    kind: KIND_GPU
  }
]
```

### 4. gRPC ì—°ê²° ì‹¤íŒ¨

#### ì¦ìƒ
```python
# TritonClient: Connection failed
```

#### í•´ê²° ë°©ë²•
```bash
# Triton Server í¬íŠ¸ í™•ì¸
docker-compose ps | grep triton

# Analysis Worker í™˜ê²½ë³€ìˆ˜ í™•ì¸
docker-compose exec analysis-worker-1 env | grep TRITON

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸
docker-compose exec analysis-worker-1 ping triton-server
```

### 5. ì¶”ë¡  ê²°ê³¼ê°€ ì´ìƒí•¨

#### ë””ë²„ê¹… ë°©ë²•
```python
# triton_client.pyì—ì„œ ë¡œê¹… ì¶”ê°€
logger.info(f"Input data: {data}")
logger.info(f"Normalized data: {normalized}")
logger.info(f"Triton output: {output_data}")
logger.info(f"Final predictions: {predictions}")
```

---

## ì„±ëŠ¥ ë¹„êµ

### í…ŒìŠ¤íŠ¸ í™˜ê²½
- GPU: Nvidia RTX 3090
- CPU: AMD Ryzen 9 5950X
- RAM: 64GB
- ëª¨ë¸: LSTM (hidden=64, layers=2)

### ê²°ê³¼

| ì§€í‘œ | ì´ì „ (ìˆ˜ë™ êµ¬í˜„) | í˜„ì¬ (Triton) | ê°œì„ ë„ |
|------|------------------|---------------|--------|
| **ì²˜ë¦¬ëŸ‰ (RPS)** | 15-20 | 80-100 | **5ë°° â¬†ï¸** |
| **ì§€ì—°ì‹œê°„ (p50)** | 50ms | 30ms | **40% â¬‡ï¸** |
| **ì§€ì—°ì‹œê°„ (p99)** | 200ms | 80ms | **60% â¬‡ï¸** |
| **GPU í™œìš©ë„** | 35% | 85% | **2.4ë°° â¬†ï¸** |
| **ë°°ì¹˜ í¬ê¸°** | ìˆ˜ë™ (max 8) | ìë™ (max 32) | âœ… |
| **ë™ì‹œ ëª¨ë¸ ì‹¤í–‰** | âŒ | âœ… | âœ… |
| **ëª¨ë‹ˆí„°ë§** | ì œí•œì  | Prometheus | âœ… |

### ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥

| ë°°ì¹˜ í¬ê¸° | ì´ì „ (Python) | í˜„ì¬ (Triton C++) | ê°œì„ ë„ |
|-----------|--------------|-------------------|--------|
| 1 | 45ms | 30ms | 1.5ë°° |
| 4 | 80ms | 35ms | 2.3ë°° |
| 8 | 140ms | 40ms | 3.5ë°° |
| 16 | 250ms | 50ms | 5.0ë°° |
| 32 | N/A (ë¯¸ì§€ì›) | 70ms | - |

**ê²°ë¡ **: Tritonì˜ Dynamic Batchingì´ ìˆ˜ë™ êµ¬í˜„ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë¹ ë¦„

---

## ë‹¤ìŒ ë‹¨ê³„

### 1. ì¶”ê°€ ìµœì í™” (ì„ íƒì‚¬í•­)

#### TensorRT ë°±ì—”ë“œ ì‚¬ìš©
```bash
# PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
python scripts/convert_to_onnx.py

# ONNXë¥¼ TensorRTë¡œ ë³€í™˜
trtexec --onnx=model.onnx --saveEngine=model.plan

# Triton config ë³€ê²½
backend: "tensorrt"
```

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**: ì¶”ê°€ 2-3ë°°

#### Model Ensemble
```protobuf
# ì „ì²˜ë¦¬ â†’ ì¶”ë¡  â†’ í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
name: "ensemble_lstm"
platform: "ensemble"

ensemble_scheduling {
  step [
    {
      model_name: "preprocess"
      model_version: -1
      input_map { ... }
      output_map { ... }
    },
    {
      model_name: "lstm_timeseries"
      model_version: -1
      input_map { ... }
      output_map { ... }
    },
    {
      model_name: "postprocess"
      model_version: -1
      input_map { ... }
      output_map { ... }
    }
  ]
}
```

### 2. A/B í…ŒìŠ¤íŒ…

```python
# ë‘ ë²„ì „ ë™ì‹œ ì„œë¹™
result_v1 = client.infer(model_name="lstm_timeseries", model_version="1", ...)
result_v2 = client.infer(model_name="lstm_timeseries", model_version="2", ...)

# ì„±ëŠ¥ ë¹„êµ
compare_predictions(result_v1, result_v2)
```

### 3. Auto-scaling (Kubernetes)

```yaml
# k8s/triton-deployment.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: triton-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: triton-server
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Nvidia Triton Inference Server ë¬¸ì„œ](https://docs.nvidia.com/deeplearning/triton-inference-server/)
- [Triton Python Backend](https://github.com/triton-inference-server/python_backend)
- [Triton Client Libraries](https://github.com/triton-inference-server/client)

### ë‚´ë¶€ ë¬¸ì„œ
- [Triton ë¶„ì„ ë¬¸ì„œ](/docs/TRITON_ANALYSIS.md)
- [ê¸°ì¡´ Model Factory (Deprecated)](/analysis-server/core/deprecated/README.md)

### ì»¤ë®¤ë‹ˆí‹°
- [Triton GitHub Issues](https://github.com/triton-inference-server/server/issues)
- [Nvidia Developer Forums](https://forums.developer.nvidia.com/)

---

**ì‘ì„±ì¼**: 2025-10-15
**ë²„ì „**: 1.0
**ì‘ì„±ì**: Analysis Team
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: Triton Inference Server 24.01
